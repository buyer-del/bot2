# ============================================
#  üîä –ê—É–¥—ñ–æ ‚Üí Google Speech-to-Text (uk-UA)
#  üñºÔ∏è –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è ‚Üí Google Vision API (OCR)
#  üß† –ê–Ω–∞–ª—ñ–∑ –∑–∞–¥–∞—á ‚Üí Vertex AI (Gemini)
# ============================================

import os
import io
import json
import subprocess
import tempfile
from typing import Optional

from google.cloud import speech_v1 as speech
from google.cloud import vision
from googleapiclient.discovery import build as gbuild  # –Ω–µ –æ–±–æ–≤'—è–∑–∫–æ–≤–æ, –∞–ª–µ —Ö–∞–π –±—É–¥–µ
# Vertex AI
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig

# -----------------------------
# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Google Credentials
# -----------------------------
def _setup_google_credentials() -> str:
    """
    –ù–∞–ª–∞—à—Ç–æ–≤—É—î GOOGLE_APPLICATION_CREDENTIALS –Ω–∞ –æ—Å–Ω–æ–≤—ñ
    –∑–º—ñ–Ω–Ω–æ—ó —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ GOOGLE_CREDENTIALS_JSON.
    –ü–æ–≤–µ—Ä—Ç–∞—î —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É credentials.
    """
    google_creds_json = os.environ.get("GOOGLE_CREDENTIALS_JSON")
    if not google_creds_json:
        raise ValueError("‚ùå GOOGLE_CREDENTIALS_JSON –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —É –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞!")

    creds_path = "/tmp/google_credentials.json"
    with open(creds_path, "w", encoding="utf-8") as f:
        f.write(google_creds_json)

    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = creds_path
    return creds_path

_CREDS_PATH = _setup_google_credentials()

# -----------------------------
# Speech/Vision –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
# -----------------------------
SPEECH_LANGUAGE = os.getenv("SPEECH_LANGUAGE", "uk-UA")

# -----------------------------
# –î–æ–ø–æ–º—ñ–∂–Ω–µ: –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è ‚Üí WAV
# -----------------------------
def _convert_to_wav_16k_mono(input_path: str) -> str:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç—É—î –±—É–¥—å-—è–∫–µ –∞—É–¥—ñ–æ/–≤—ñ–¥–µ–æ –≤ WAV PCM 16-bit, mono, 16000 Hz.
    –í–∏–º–∞–≥–∞—î –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ ffmpeg.
    """
    fd, out_path = tempfile.mkstemp(suffix=".wav")
    os.close(fd)
    cmd = [
        "ffmpeg",
        "-hide_banner", "-loglevel", "error",
        "-y",
        "-i", input_path,
        "-vn",
        "-ac", "1",
        "-ar", "16000",
        "-sample_fmt", "s16",
        out_path,
    ]
    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        try:
            os.remove(out_path)
        except Exception:
            pass
        raise RuntimeError(f"ffmpeg: –ø–æ–º–∏–ª–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó ({e})")
    return out_path

# -----------------------------
# Google Speech-to-Text
# -----------------------------
def transcribe_audio(input_path: str) -> Optional[str]:
    wav_path = None
    try:
        wav_path = _convert_to_wav_16k_mono(input_path)
        with open(wav_path, "rb") as f:
            content = f.read()

        audio = speech.RecognitionAudio(content=content)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code=SPEECH_LANGUAGE,
            enable_automatic_punctuation=True,
            model="latest_long",
        )

        client = speech.SpeechClient()
        response = client.recognize(config=config, audio=audio)
        if not response.results:
            return None

        parts = []
        for result in response.results:
            if result.alternatives:
                parts.append(result.alternatives[0].transcript)
        text = " ".join(t.strip() for t in parts if t and t.strip())
        return text or None
    except Exception:
        return None
    finally:
        if wav_path and os.path.exists(wav_path):
            try:
                os.remove(wav_path)
            except Exception:
                pass

# -----------------------------
# Google Vision OCR
# -----------------------------
def extract_text_from_image(image_path: str) -> Optional[str]:
    try:
        client = vision.ImageAnnotatorClient()
        with open(image_path, "rb") as image_file:
            content = image_file.read()
        image = vision.Image(content=content)
        response = client.text_detection(image=image)
        if response.error.message:
            raise RuntimeError(f"Vision API error: {response.error.message}")
        if not response.text_annotations:
            return None
        full_text = (response.text_annotations[0].description or "").strip()
        return full_text or None
    except Exception:
        return None

# -----------------------------
# Vertex AI (Gemini) ‚Äî –∞–Ω–∞–ª—ñ–∑ –∑–∞–¥–∞—á
# -----------------------------
def _init_vertex() -> tuple[str, str]:
    """
    –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î Vertex AI.
    –ü–æ–≤–µ—Ä—Ç–∞—î (project_id, location).
    """
    project_id = None
    try:
        with open(_CREDS_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            project_id = data.get("project_id")
    except Exception:
        pass

    if not project_id:
        # –∑–∞–ø–∞—Å–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç: –º–æ–∂–Ω–∞ –∑–∞–¥–∞—Ç–∏ —á–µ—Ä–µ–∑ env
        project_id = os.getenv("GOOGLE_PROJECT_ID")

    location = os.getenv("VERTEX_LOCATION", "us-central1")
    if not project_id:
        raise ValueError("–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ project_id –¥–ª—è Vertex AI. –î–æ–¥–∞–π GOOGLE_PROJECT_ID –∞–±–æ project_id —É –∫–ª—é—á—ñ.")

    vertexai.init(project=project_id, location=location)
    return project_id, location

def analyze_task_with_ai(prompt: str, raw_text: str, timeout_sec: int = 20) -> Optional[str]:
    """
    –ü—Ä–∏–π–º–∞—î —Å–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º—Ç + —Å–∏—Ä–∏–π —Ç–µ–∫—Å—Ç —á–æ—Ä–Ω–µ—Ç–∫–∏.
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç —É —Ñ–æ—Ä–º–∞—Ç—ñ:
      –ù–∞–∑–≤–∞: ...
      –¢–µ–≥: ...
      –î–µ–¥–ª–∞–π–Ω: ...
      –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: ...
      –û–ø–∏—Å: ...
    –∞–±–æ None, —è–∫—â–æ —â–æ—Å—å –ø—ñ—à–ª–æ –Ω–µ —Ç–∞–∫.
    """
    try:
        _init_vertex()
        model_name = os.getenv("VERTEX_MODEL", "gemini-1.5-flash")
        model = GenerativeModel(model_name)

        # –ü—Ä–æ—Å–∏–º–æ —Å—Ç—Ä–æ–≥–∏–π —Ñ–æ—Ä–º–∞—Ç —ñ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É
        system = (
            prompt
            + "\n\n–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é. "
            + "–ü–æ–≤–µ—Ä–Ω–∏ –ª–∏—à–µ –ø'—è—Ç—å —Ä—è–¥–∫—ñ–≤ —Å—Ç—Ä–æ–≥–æ —É —Ñ–æ—Ä–º—ñ:\n"
            + "–ù–∞–∑–≤–∞: ...\n–¢–µ–≥: ...\n–î–µ–¥–ª–∞–π–Ω: ...\n–ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: ...\n–û–ø–∏—Å: ..."
        )

        generation_config = GenerationConfig(
            temperature=0.2,
            top_p=0.9,
            max_output_tokens=512,
        )

        # –ö–æ–Ω—Ç–µ–Ω—Ç: —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è + –≤–∏—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç
        parts = [
            {"role": "user", "parts": [system]},
            {"role": "user", "parts": [f"–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è:\n{raw_text}"]},
        ]

        resp = model.generate_content(
            parts,
            generation_config=generation_config,
            timeout=timeout_sec,
        )
        if not resp or not getattr(resp, "text", None):
            return None

        answer = (resp.text or "").strip()
        if not answer:
            return None
        return answer
    except Exception:
        return None
